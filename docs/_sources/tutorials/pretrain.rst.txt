Pretrained Molecular Representations
====================================

.. include:: ../bibliography.rst

For the property prediction task in drug discovery, due to the high expense,
pre-training is a special type of problems in drug discovery tasks.


Self-Supervised Pre-training
----------------------------

The self-supervised pre-training belongs to the unsupervised pre-training. It is
unsupervised because no supervised signal, i.e., the molecular drug property, is
provided. It is called the self-supervised because it constructs the positive and
negative views from the data itself. And specifically to the self-supervised
pre-training for molecule property prediction, it explores the structure information
of molecular data from different aspects as illustrated below.

Infograph
^^^^^^^^^

`InfoGraph`_ (IG) sets the node- and graph- representation from the same graph as positive pairs,
while those from different graphs as negative pairs.

.. code:: python

    import torch
    from torch import nn, optim
    from torch.utils import data as torch_data

    from drugdiscovery import core, data, datasets, tasks, models


    if __name__ == "__main__":
        dataset = datasets.ClinTox("datasets", node_feature="pretraining",
                                   edge_feature="pretraining")

        lengths = [len(dataset), 0, 0]
        train_set, _, _ = torch_data.random_split(dataset, lengths)

        gin_model = models.GIN(GIN_version="pretraining", emb_dim=300, num_layer=5,
                               readout="mean")
        model = models.InfoGraph(gin_model, separate_model=False)

        task = tasks.Unsupervised(model)
        optimizer = optim.Adam(task.parameters(), lr=1e-3)
        solver = core.Engine(task, train_set, None, None, optimizer,
                             gpus=[0], batch_size=256)

        solver.train(num_epoch=100)

The above is the toy example on `ClinTox`_ dataset, and the output is similar to
the following.

.. code:: bash

    average graph-node mutual information: 1.30305

.. image:: ../../../asset/graph/infograph.png

Attribute Masking
^^^^^^^^^^^^^^^^^

`Attribute Masking`_ (AM) predicts the atom node type from its neighbors. It can
be seen as self-supervised by viewing the actual node type as positive, and all
the other node types as negative.

.. code:: python

    from torch import nn, optim
    from torch.utils import data as torch_data
    from drugdiscovery import core, datasets, tasks, models

    dataset = datasets.ClinTox("datasets", node_feature="pretraining",
                               edge_feature="pretraining")

    lengths = [len(dataset), 0, 0]
    train_set, _, _ = torch_data.random_split(dataset, lengths)

    model = models.GIN(GIN_version="pretraining", emb_dim=300, num_layer=5,
                       readout="mean")
    atom_attribute_model = nn.Linear(300, 119)
    parameters = list(model.parameters()) + list(atom_attribute_model.parameters())
    task = tasks.AttributeMasking(model, atom_attribute_model, 0.15)

    optimizer = optim.Adam(parameters, lr=1e-3)
    solver = core.Engine(task, train_set, None, None, optimizer,
                         gpus=[0], batch_size=256)

    solver.train(num_epoch=100)

The above is the toy example on `ClinTox`_ dataset, and the output is similar to
the following.

.. code:: bash

    average CrossEntropyLoss() loss: 0.285154
    average acc: 0.90249

.. image:: ../../../asset/graph/attrmasking.png

We also include other mainstream self-supervised pre-training methods as below:

.. seealso::
    :func:`Edge Prediction (EP) <drugdiscovery.tasks.EdgePrediction>`
    :func:`Context Prediction (CP) <drugdiscovery.tasks.ContextPrediction>`
