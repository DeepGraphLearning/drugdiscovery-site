Knowledge Graph Reasoning
=========================

.. include:: ../bibliography.rst

In knowledge graphs, one important task is knowledge graph reasoning, which aims at predicting missing (h,r,t)-links given existing (h,r,t)-links in a knowledge graph. For knowledge graph reasoning, one popular kind of method is the knowledge graph embedding method. The basic idea is to learn an embedding vector for each entity and relation in a knowledge graph based on existing (h,r,t)-links. Then these embeddings are further used to predict missing links.

In this tutorial, we provide an example to illustrate how to use Drugdiscovery for knowledge graph reasoning.

Prepare the Dataset
-------------------

We use the `FB15k-237`_ dataset for illustration. `FB15k-237`_ is constructed from Freebase, and the dataset has 14,541 entities as well as 237 relations. For the dataset, there is a standard split of training/validation/test sets. We can load the dataset using the following code:

.. code:: python

    import torch
    from drugdiscovery import core, datasets, tasks, models

    dataset = datasets.FB15k237("~/kg-datasets/")
    train_set, valid_set, test_set = dataset.split()

Define our Model
----------------

Once we load the dataset, we are ready to build the model. Let's take the RotatE model as an example, we can use the following code for model construction.

.. code:: python

    model = models.RotatE(num_entity=dataset.num_entity, num_relation=dataset.num_relation, embedding_dim=2048, max_score=9)

Here, ``embedding_dim`` specifies the dimension of entity and relation embeddings. ``max_score`` specifies the bias for inferring the plausibility of a (h,r,t) triplet.

You may consider using a smaller embedding dimension for better efficiency.

Afterwards, we further need to define our task. For the knowledge graph embedding task, we can simply use the following code.

.. code:: python

    task = tasks.KnowledgeGraphEmbedding(model, num_negative=256, adversarial_temperature=1)

Here, ``num_negative`` is the number of negative examples used for training, and ``adversarial_temperature`` is the temperature for sampling negative examples.

Train and Test
--------------

Afterwards, we can now train and test our model. For model training, we need to set up an optimizer and put everything together into an Engine instance with the following code.

.. code:: python

    optimizer = torch.optim.Adam(task.parameters(), lr=2e-5)
    solver = core.Engine(task, train_set, valid_set, test_set, optimizer, gpus=[0], batch_size=1024)
    solver.train(num_epoch=200)

Here, we can reduce ``num_epoch`` for better efficiency.

Afterwards, we may further evaluate the model on the validation set using the following code.

.. code:: python

    solver.evaluate("valid")

